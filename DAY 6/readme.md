# Data Engineering Concepts and Hadoop Overview

This document summarizes key data engineering concepts, terminologies, and an overview of Hadoop and related technologies.

---

## Core Building Blocks of Data Engineering

Data engineering involved several essential areas that worked together to transform raw data into usable information:

- **Data Storage:** Learned about storage solutions such as data warehouses, databases, and cloud storage.
- **Data Ingestion:** Learned how data was collected from various sources including databases, APIs, and sensors.
- **Data Transformation:** Learned the processes for cleaning, transforming, and enriching raw data for analysis.
- **Data Modeling:** Learned how data structures and relationships were designed for consistency and usability.
- **Data Pipelines:** Learned about automated workflows that managed data movement using tools like Apache Airflow and Kafka.
- **Data Quality:** Learned methods to ensure data accuracy, consistency, and completeness.
- **Data Governance:** Learned about rules and policies for managing data access, usage, and retention.

---

## Terminologies in Data Engineering

### Data Types & Sources
- Structured Data (SQL databases)
- Semi-Structured Data (JSON, XML, Parquet)
- Unstructured Data (text files, images, videos)
- Data Lake and Data Warehouse concepts
- OLTP (transaction processing) and OLAP (analytics)

### Data Processing
- ETL and ELT pipelines
- Batch and Stream Processing (Spark, Kafka, Flink)
- Data Ingestion and Orchestration (Airflow)

### Storage & Formats
- Columnar Storage (Parquet, ORC)
- Row-based Storage (CSV, JSON)
- Distributed File Systems (HDFS, S3)
- Data Partitioning and Sharding

### Query Engines
- Presto/Trino, Hive, BigQuery, Athena

### Governance & Quality
- Data Catalogs (Apache Atlas)
- Data Lineage and Profiling
- Schema Evolution

---

## Hadoop and Big Data Technologies

- **Hadoop:** Learned that Hadoop was a framework for distributed storage and processing of large datasets.
- **MapReduce:** Learned how MapReduce split processing into Map and Reduce phases to handle batch jobs.
- **Apache Spark:** Learned why Spark improved on MapReduce by enabling faster, in-memory, real-time, and iterative processing with a more flexible API.

---
